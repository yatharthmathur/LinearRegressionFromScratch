{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='RED' size='6'><b>MACHINE LEARNING : Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1.jpeg\" alt=\"Linear Regression Model\" height='400' width='400' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Linear Regression is the most basic approach of modelling used in predictive analysis.\n",
    "<br>The overall idea of regression is to examine two things:<br>(1) does a set of predictor variables do a good job in predicting an outcome (dependent) variable?  <br>(2) Which variables in particular are significant predictors of the outcome variable, and in what way do they impact the outcome variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <font size='3'>Some basic concepts : </font><br><br>\n",
    "    Supervised learning </b>: Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.<br><br>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <font size='3'>Model Representation</font> <br><br></b>\n",
    "    First, the goal of most machine learning algorithms is to construct a model: a hypothesis that can be used to estimate Y based on X. The hypothesis, or model, maps inputs to outputs.<br><br>\n",
    "    The hypothesis is usually presented as :\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='5' align=left>$$h_\\theta(x^i) = \\theta_0 + x^i\\theta_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to find the values of <font size='3'>ùúÉ</font>0 and <font size='3'>ùúÉ</font>1 such that the cost(h(x)) for each ith test sample(x^i) has the minimum mean difference from respective  training example y^i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This boils down to minimization of the cost function for m training samples as follows : \n",
    "\n",
    "<font size='5'>$$\\frac{1}{2m} {\\textstyle \\sum^m_i} (h_\\theta(x)_i - y_i)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font size='5' color='red'>IMPLEMENTATION OF LINEAR REGRESSION FOR MACHINE LEARNING IN PYTHON</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green' size='5'><b>STEP 1 </b>: Importing the appropriate libraries.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a self explanatory code. We're basically importing all the necessary libraries needed for the implementation of Linear Regression model, that is numpy for mathematics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green' size='5'><b>STEP 2 </b>: Reading data and creating the required vectors/matrices</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([-1.0, -2.0, -3.0, -4.0, -5.0])\n",
    "Y = data\n",
    "X = [0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "x_train = [5.0, 6.0, 7.0, 8.0, 9.0]\n",
    "y_train = [-6.0, -7.0, -8.0, -9.0, -10.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically this data represents a line y = -x-1, real life data set may not perfectly fit a line as such but this will suffice as an introduction to linear regression and optimizing algorithms (Gradient Descent in our case).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green' size='5'><b>STEP 3 </b>: Making the cost function</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#let the prediction be Y = W * X + b, the answer should be y= -1*X + -1\n",
    "W = 0.3\n",
    "b = -0.3\n",
    "ones = np.array([1.0,1.0,1.0,1.0,1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(W,b):\n",
    "    pred = np.dot(X,W) + np.dot(b,ones)\n",
    "    return (1/10.0)*np.sum(np.square(pred-Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.135000000000001"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(-0.3,0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know the current cost is really high, optimally we would want the parameters W and b such that the cost is 0 as follow. So we have to get W = -1 and b = -1 for our model to work accurately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(-1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green' size='5'><b>STEP 4 </b>: Gradient Descent to optimize W and b or Training our model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is basically to optimize our W and b such that the cost (or loss) function has the minimum mean squared deviation. The following code will run Gradient Descent to find out the optimal values for W and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GradientDescent(X,Y,W,b,a,i):\n",
    "    cost_history = np.zeros(i)\n",
    "    W_history = np.zeros(i)\n",
    "    b_history = np.zeros(i)\n",
    "    w1 = W\n",
    "    b1 = b\n",
    "    for it in range(i):\n",
    "        W_history[it] = W\n",
    "        b_history[it] = b\n",
    "        pred = np.dot(X,W) + np.dot(b,ones)\n",
    "        w1 = W - (1/m)*a*np.dot(np.transpose(X),(pred-Y))\n",
    "        b1 = b - (1/m)*a*np.sum(pred-Y)\n",
    "        cost_history[it] = cost(w1,b1)\n",
    "        W = w1\n",
    "        b = b1\n",
    "        \n",
    "    return W, b, W_history, b_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b, W_h, b_h = GradientDescent(X,Y,W,b,0.1,120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have optimized our parameters W and b let's see how close they are to our required value : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0000000038180321 -0.9999999891156264\n",
      "Cost on the current parameters =  1.9853126266030124e-17\n"
     ]
    }
   ],
   "source": [
    "print(W,b)\n",
    "print(\"Cost on the current parameters = \", cost(W,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that it is pretty close. Now we can use this code to determine any univariate linear regression problem with a very few adjustments. Let's see how it predicts :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green' size='5'><b>STEP 5 </b>: Testing the model using testing data </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.dot(W,x_train) + np.dot(ones,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -6.00000001  -7.00000001  -8.00000002  -9.00000002 -10.00000002]\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out the accuracy on the given test data : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error =  1.40059486408749e-15\n"
     ]
    }
   ],
   "source": [
    "mean_squared_error=np.mean(np.sum(np.square(pred-y_train)))\n",
    "print(\"Mean Squared Error = \", mean_squared_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the mean squared error is very low, this means our model is extremely accurate for the given test data. Since we chose a data that fits a line itself, the model can be trained to maximum accuracy but for realistic data such idealism is non-existent and accuracies will fall to below 90% as well. This was a simple tutorial to understand how linear regression works and shall not be taken as an implementation of its use case.\n",
    "\n",
    "Note : The above code can be altered very minutely to fit any data and implement Linear Regression model to predict values.\n",
    "\n",
    "For further reading and understanding refer (Andrew Ng's Tutorial on YouTube): \n",
    "https://www.youtube.com/watch?v=PPLop4L2eGk&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN\n",
    "\n",
    "This is basically the source and inspiration of all the above implemented knowledge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
